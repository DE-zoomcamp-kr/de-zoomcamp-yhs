{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d13fee",
   "metadata": {},
   "source": [
    "# Module 6 Homework - Spark Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89ce833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, unix_timestamp, max\n",
    "from pyspark.sql.types import *\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7532ea52",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b716129c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yellow_tripdata_2025-11.parquet already exists.\n",
      "taxi_zone_lookup.csv already exists.\n"
     ]
    }
   ],
   "source": [
    "yellow_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-11.parquet\"\n",
    "zone_url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\"\n",
    "\n",
    "yellow_file = \"yellow_tripdata_2025-11.parquet\"\n",
    "zone_file = \"taxi_zone_lookup.csv\"\n",
    "\n",
    "def download_file(url, filename):\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        r = requests.get(url)\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        print(\"Done.\")\n",
    "    else:\n",
    "        print(f\"{filename} already exists.\")\n",
    "\n",
    "download_file(yellow_url, yellow_file)\n",
    "download_file(zone_url, zone_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0d3176",
   "metadata": {},
   "source": [
    "## Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f5d6b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/03/01 10:55:41 WARN Utils: Your hostname, codespaces-58a1f5, resolves to a loopback address: 127.0.0.1; using 10.0.12.62 instead (on interface eth0)\n",
      "26/03/01 10:55:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/03/01 10:55:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spark Version:\n",
      "4.1.1\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"DE-Zoomcamp-HW6\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"\\nSpark Version:\")\n",
    "print(spark.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114d5e7c",
   "metadata": {},
   "source": [
    "## Read Yellow Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b672b221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Schema:\n",
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      " |-- cbd_congestion_fee: double (nullable = true)\n",
      "\n",
      "\n",
      "Total Records:\n",
      "4181444\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(yellow_file)\n",
    "\n",
    "print(\"\\nSchema:\")\n",
    "df.printSchema()\n",
    "\n",
    "print(\"\\nTotal Records:\")\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dddd14",
   "metadata": {},
   "source": [
    "## Question 2 - Repartition & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2cf7c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 2 - Average Parquet File Size (MB):\n",
      "25.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "output_path = \"yellow_2025_11_repartitioned\"\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    shutil.rmtree(output_path)\n",
    "\n",
    "df_repartitioned = df.repartition(4)\n",
    "df_repartitioned.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "# Calculate average file size\n",
    "parquet_files = [\n",
    "    os.path.join(output_path, f)\n",
    "    for f in os.listdir(output_path)\n",
    "    if f.endswith(\".parquet\")\n",
    "]\n",
    "\n",
    "sizes_mb = [os.path.getsize(f) / (1024 * 1024) for f in parquet_files]\n",
    "avg_size = sum(sizes_mb) / len(sizes_mb)\n",
    "\n",
    "print(\"\\nQuestion 2 - Average Parquet File Size (MB):\")\n",
    "print(round(avg_size, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c3707c",
   "metadata": {},
   "source": [
    "## Question 3 - Trips on Nov 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ab8b921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 3 - Trips on 2025-11-15:\n",
      "162604\n"
     ]
    }
   ],
   "source": [
    "df_15 = df.filter(\n",
    "    to_date(col(\"tpep_pickup_datetime\")) == \"2025-11-15\"\n",
    ")\n",
    "\n",
    "count_15 = df_15.count()\n",
    "\n",
    "print(\"\\nQuestion 3 - Trips on 2025-11-15:\")\n",
    "print(count_15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18573e8",
   "metadata": {},
   "source": [
    "## Question 4 - Longest Trip (hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2c15b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 4 - Longest Trip (hours):\n",
      "90.65\n"
     ]
    }
   ],
   "source": [
    "df_duration = df.withColumn(\n",
    "    \"duration_hours\",\n",
    "    (unix_timestamp(\"tpep_dropoff_datetime\") -\n",
    "     unix_timestamp(\"tpep_pickup_datetime\")) / 3600\n",
    ")\n",
    "\n",
    "max_duration = df_duration.agg(\n",
    "    max(\"duration_hours\")\n",
    ").collect()[0][0]\n",
    "\n",
    "print(\"\\nQuestion 4 - Longest Trip (hours):\")\n",
    "print(round(max_duration, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9c588b",
   "metadata": {},
   "source": [
    "## Question 5 - Spark UI Port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ba44d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 5 - Spark UI URL:\n",
      "http://e4576300-65f6-46ea-a666-de14ee3b66e8.internal.cloudapp.net:4040\n"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "\n",
    "ui_url = sc.uiWebUrl\n",
    "\n",
    "print(\"\\nQuestion 5 - Spark UI URL:\")\n",
    "print(ui_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1790b514",
   "metadata": {},
   "source": [
    "## Question 6 - Least Frequent Pickup Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e46a921a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 6 - Least Frequent Pickup Zone:\n",
      "Governor's Island/Ellis Island/Liberty Island\n",
      "\n",
      "=================================\n",
      "Homework Execution Complete.\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "zones = spark.read.option(\"header\", \"true\").csv(zone_file)\n",
    "\n",
    "joined = df.join(\n",
    "    zones,\n",
    "    df.PULocationID == zones.LocationID,\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "least_zone = joined.groupBy(\"Zone\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"count\") \\\n",
    "    .limit(1) \\\n",
    "    .collect()[0][\"Zone\"]\n",
    "\n",
    "print(\"\\nQuestion 6 - Least Frequent Pickup Zone:\")\n",
    "print(least_zone)\n",
    "\n",
    "print(\"\\n=================================\")\n",
    "print(\"Homework Execution Complete.\")\n",
    "print(\"=================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882d193b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
